"""
Streamlit App for Question Management

Interactive interface for viewing and managing parsed questions.
"""

import streamlit as st
import pandas as pd
import os
import sys
from typing import List, Dict, Any
import json
from datetime import datetime

# Add current directory to path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

# Page config
st.set_page_config(
    page_title="LaTeX Question Parser - Dashboard",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Force set upload size limit at runtime
# Override upload size limit
try:
    import streamlit.web.server.server as server
    server.UPLOAD_FILE_SIZE_LIMIT_MB = 1024  # 1GB
except:
    pass

try:
    import streamlit.config as config
    config._set_option('server.maxUploadSize', 1024, 'manual')
    config._set_option('server.maxMessageSize', 1024, 'manual')
except:
    pass

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .question-card {
        background-color: #ffffff;
        padding: 1.5rem;
        border-radius: 0.5rem;
        border: 1px solid #e0e0e0;
        margin-bottom: 1rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .status-active {
        background-color: #d4edda;
        color: #155724;
        padding: 0.25rem 0.5rem;
        border-radius: 0.25rem;
        font-weight: bold;
    }
    .status-pending {
        background-color: #fff3cd;
        color: #856404;
        padding: 0.25rem 0.5rem;
        border-radius: 0.25rem;
        font-weight: bold;
    }
</style>
""", unsafe_allow_html=True)

def clean_text_display(text):
    """Clean text for better display in Streamlit (for processed content)."""
    if pd.isna(text) or text is None:
        return "N/A"

    text = str(text)

    # Handle JSON strings (for answers field)
    import json
    import re

    # Try to parse as JSON and format nicely (for non-answers fields)
    if text.startswith('[') and text.endswith(']'):
        try:
            parsed = json.loads(text)
            if isinstance(parsed, list):
                formatted_items = []
                for i, item in enumerate(parsed):
                    if isinstance(item, dict):
                        content = item.get('content', '')
                        # Don't add ‚úì mark here - this is handled by format_answers_display for answers
                        formatted_items.append(f"{i+1}. {content}")
                    else:
                        formatted_items.append(f"{i+1}. {item}")
                text = '\n'.join(formatted_items)
        except:
            pass  # If not valid JSON, continue with text cleaning

    # Decode Unicode escape sequences safely
    import codecs
    try:
        # Only decode if it contains unicode escapes
        if '\\u' in text:
            text = codecs.decode(text, 'unicode_escape')
    except:
        pass  # If decoding fails, continue with original text

    # Handle common LaTeX/encoding issues
    replacements = {
        '\\n': '\n',
        '\\t': '    ',  # Convert tabs to spaces
        '\\\\': '\n',   # LaTeX line breaks
        '\\lq\\lq': '"',
        '\\rq\\rq': '"',
        '\\lq': "'",
        '\\rq': "'",
        '\\$': '$',
        '\\{': '{',
        '\\}': '}',
        '\\&': '&',
        '\\%': '%',
        '\\#': '#',
        '\\_': '_',
        '\\^': '^',
        '\\~': '~',
        '\\textbf{': '**',
        '\\textit{': '*',
        '\\emph{': '*',
        '\\begin{itemize}': '\n',
        '\\end{itemize}': '\n',
        '\\item': '‚Ä¢ ',
        '\\begin{enumerate}': '\n',
        '\\end{enumerate}': '\n',
        '\\begin{itemchoice}': '\n',
        '\\end{itemchoice}': '\n',
        '\\itemch': '‚Ä¢ ',
        '\\True': '‚úì ',
        '\\False': '‚úó ',
    }

    for old, new in replacements.items():
        text = text.replace(old, new)

    # Clean up some LaTeX commands but preserve math
    # Keep math expressions like \dfrac, \frac, \sqrt, etc.
    math_commands = ['dfrac', 'frac', 'sqrt', 'infty', 'sum', 'int', 'lim', 'sin', 'cos', 'tan', 'log', 'ln']

    # Don't remove math commands - they should be preserved
    # Only remove some formatting commands
    formatting_commands = ['textbf', 'textit', 'emph']
    for cmd in formatting_commands:
        text = re.sub(rf'\\{cmd}\{{([^}}]*)\}}', r'\1', text)  # Remove formatting but keep content

    # Clean up multiple spaces and newlines
    text = re.sub(r'\n\s*\n', '\n\n', text)  # Multiple newlines to double
    text = re.sub(r' +', ' ', text)  # Multiple spaces to single
    text = re.sub(r'\t+', '    ', text)  # Multiple tabs to spaces

    return text.strip()

def format_answers_display(text):
    """Format answers for display - already in semicolon-separated text format."""
    if pd.isna(text) or text is None:
        return "N/A"

    text = str(text)

    # If it's still JSON format (backward compatibility), parse it
    if text.startswith('[') and text.endswith(']'):
        try:
            import json
            parsed = json.loads(text)
            if isinstance(parsed, list):
                formatted_items = []
                for item in parsed:
                    if isinstance(item, dict):
                        content = item.get('content', '')
                        # Decode Unicode escape sequences
                        import codecs
                        try:
                            if '\\u' in content:
                                content = codecs.decode(content, 'unicode_escape')
                        except:
                            pass
                        formatted_items.append(content)
                    else:
                        formatted_items.append(str(item))
                return '; '.join(formatted_items)
        except:
            pass  # If not valid JSON, continue with text cleaning

    # For new text format, just return as is (already semicolon-separated)
    # Convert double backslashes back to single for display
    text = text.replace('\\\\', '\\')

    return text.strip()

def clean_raw_content_display(text):
    """Clean raw content and solution for display - preserve LaTeX commands."""
    if pd.isna(text) or text is None:
        return "N/A"

    text = str(text)

    # Only basic cleaning for raw content - preserve all LaTeX commands
    # Decode Unicode escape sequences safely
    import codecs
    import re
    try:
        if '\\u' in text:
            text = codecs.decode(text, 'unicode_escape')
    except:
        pass

    # Only handle basic escape sequences, preserve LaTeX commands
    basic_replacements = {
        '\\n': '\n',
        '\\t': '    ',
        '\\\\': '\n',   # LaTeX line breaks to newlines
    }

    for old, new in basic_replacements.items():
        text = text.replace(old, new)

    # Clean up multiple spaces and newlines
    text = re.sub(r'\n\s*\n', '\n\n', text)  # Multiple newlines to double
    text = re.sub(r' +', ' ', text)  # Multiple spaces to single

    return text.strip()

@st.cache_data
def load_data():
    """Load data from CSV files."""
    try:
        # Load questions
        questions_path = "output/questions.csv"
        if os.path.exists(questions_path):
            questions_df = pd.read_csv(questions_path, encoding='utf-8')
        else:
            # Return empty DataFrames instead of None to allow dashboard to work
            questions_df = pd.DataFrame()
        
        # Load question codes
        codes_path = "output/question_codes.csv"
        if os.path.exists(codes_path):
            codes_df = pd.read_csv(codes_path)
        else:
            codes_df = pd.DataFrame()
        
        # Load question tags
        tags_path = "output/question_tags.csv"
        if os.path.exists(tags_path):
            tags_df = pd.read_csv(tags_path)
        else:
            tags_df = pd.DataFrame()
        
        return questions_df, codes_df, tags_df
    
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return None, None, None

def create_filters(questions_df, codes_df):
    """Create filter sidebar."""
    st.sidebar.header("üîç Filters")
    
    filters = {}
    
    # Question Type filter
    if 'type' in questions_df.columns:
        types = ['All'] + sorted(questions_df['type'].dropna().unique().tolist())
        filters['type'] = st.sidebar.selectbox("Question Type", types)
    
    # Status filter
    if 'status' in questions_df.columns:
        statuses = ['All'] + sorted(questions_df['status'].dropna().unique().tolist())
        filters['status'] = st.sidebar.selectbox("Status", statuses)
    
    # Difficulty filter
    if 'difficulty' in questions_df.columns:
        difficulties = ['All'] + sorted(questions_df['difficulty'].dropna().unique().tolist())
        filters['difficulty'] = st.sidebar.selectbox("Difficulty", difficulties)
    
    # Creator filter
    if 'creator' in questions_df.columns:
        creators = ['All'] + sorted(questions_df['creator'].dropna().unique().tolist())
        filters['creator'] = st.sidebar.selectbox("Creator", creators)
    
    # QuestionCode filters (if codes available)
    if not codes_df.empty:
        st.sidebar.subheader("üìã Question Code Filters")
        
        # Grade filter
        if 'grade' in codes_df.columns:
            grades = ['All'] + sorted(codes_df['grade'].dropna().unique().tolist())
            filters['grade'] = st.sidebar.selectbox("Grade", grades)
        
        # Subject filter
        if 'subject' in codes_df.columns:
            subjects = ['All'] + sorted(codes_df['subject'].dropna().unique().tolist())
            filters['subject'] = st.sidebar.selectbox("Subject", subjects)
        
        # Level filter
        if 'level' in codes_df.columns:
            levels = ['All'] + sorted(codes_df['level'].dropna().unique().tolist())
            filters['level'] = st.sidebar.selectbox("Level", levels)
        
        # Chapter filter
        if 'chapter' in codes_df.columns:
            chapters = ['All'] + sorted(codes_df['chapter'].dropna().unique().tolist())
            filters['chapter'] = st.sidebar.selectbox("Chapter", chapters)
    
    # Search
    st.sidebar.subheader("üîé Search")
    filters['search'] = st.sidebar.text_input("Search in content", "")
    
    # Advanced filters
    with st.sidebar.expander("‚öôÔ∏è Advanced Filters"):
        # Has solution filter
        filters['has_solution'] = st.selectbox("Has Solution", ['All', 'Yes', 'No'])
        
        # Usage count range
        if 'usageCount' in questions_df.columns:
            max_usage = int(questions_df['usageCount'].max()) if questions_df['usageCount'].max() > 0 else 100
            filters['usage_range'] = st.slider("Usage Count Range", 0, max_usage, (0, max_usage))
        
        # Feedback range
        if 'feedback' in questions_df.columns:
            max_feedback = int(questions_df['feedback'].max()) if questions_df['feedback'].max() > 0 else 10
            filters['feedback_range'] = st.slider("Feedback Range", 0, max_feedback, (0, max_feedback))
    
    return filters

def apply_filters(questions_df, codes_df, filters):
    """Apply filters to the dataframe."""
    filtered_df = questions_df.copy()
    
    # Apply question filters
    if filters.get('type') and filters['type'] != 'All':
        filtered_df = filtered_df[filtered_df['type'] == filters['type']]
    
    if filters.get('status') and filters['status'] != 'All':
        filtered_df = filtered_df[filtered_df['status'] == filters['status']]
    
    if filters.get('difficulty') and filters['difficulty'] != 'All':
        filtered_df = filtered_df[filtered_df['difficulty'] == filters['difficulty']]
    
    if filters.get('creator') and filters['creator'] != 'All':
        filtered_df = filtered_df[filtered_df['creator'] == filters['creator']]
    
    # Apply QuestionCode filters
    if not codes_df.empty and 'questionCodeId' in filtered_df.columns:
        code_filters = {}
        
        if filters.get('grade') and filters['grade'] != 'All':
            code_filters['grade'] = filters['grade']
        
        if filters.get('subject') and filters['subject'] != 'All':
            code_filters['subject'] = filters['subject']
        
        if filters.get('level') and filters['level'] != 'All':
            code_filters['level'] = filters['level']
        
        if filters.get('chapter') and filters['chapter'] != 'All':
            code_filters['chapter'] = filters['chapter']
        
        if code_filters:
            # Filter codes first
            filtered_codes = codes_df.copy()
            for field, value in code_filters.items():
                filtered_codes = filtered_codes[filtered_codes[field] == value]
            
            # Get matching question IDs
            valid_code_ids = filtered_codes['code'].tolist()
            filtered_df = filtered_df[filtered_df['questionCodeId'].isin(valid_code_ids)]
    
    # Apply search filter
    if filters.get('search'):
        search_term = filters['search'].lower()
        mask = (
            filtered_df['content'].str.lower().str.contains(search_term, na=False) |
            filtered_df['solution'].str.lower().str.contains(search_term, na=False)
        )
        filtered_df = filtered_df[mask]
    
    # Apply advanced filters
    if filters.get('has_solution') and filters['has_solution'] != 'All':
        if filters['has_solution'] == 'Yes':
            filtered_df = filtered_df[filtered_df['solution'].notna() & (filtered_df['solution'] != '')]
        else:
            filtered_df = filtered_df[filtered_df['solution'].isna() | (filtered_df['solution'] == '')]
    
    if filters.get('usage_range'):
        min_usage, max_usage = filters['usage_range']
        filtered_df = filtered_df[
            (filtered_df['usageCount'] >= min_usage) & 
            (filtered_df['usageCount'] <= max_usage)
        ]
    
    if filters.get('feedback_range'):
        min_feedback, max_feedback = filters['feedback_range']
        filtered_df = filtered_df[
            (filtered_df['feedback'] >= min_feedback) & 
            (filtered_df['feedback'] <= max_feedback)
        ]
    
    return filtered_df

def display_statistics(questions_df, filtered_df):
    """Display statistics dashboard."""
    st.subheader("üìä Statistics")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Questions", len(questions_df))
    
    with col2:
        st.metric("Filtered Questions", len(filtered_df))
    
    with col3:
        active_count = len(filtered_df[filtered_df['status'] == 'ACTIVE']) if 'status' in filtered_df.columns else 0
        st.metric("Active Questions", active_count)
    
    with col4:
        pending_count = len(filtered_df[filtered_df['status'] == 'PENDING']) if 'status' in filtered_df.columns else 0
        st.metric("Pending Questions", pending_count)
    
    # Charts
    if len(filtered_df) > 0:
        col1, col2 = st.columns(2)
        
        with col1:
            if 'type' in filtered_df.columns:
                type_counts = filtered_df['type'].value_counts()
                st.bar_chart(type_counts)
                st.caption("Questions by Type")
        
        with col2:
            if 'status' in filtered_df.columns:
                status_counts = filtered_df['status'].value_counts()
                st.bar_chart(status_counts)
                st.caption("Questions by Status")

def process_uploaded_file(uploaded_file):
    """Process uploaded LaTeX file and return parsed data."""
    if uploaded_file is None:
        return None, None, None

    try:
        # Save uploaded file temporarily
        import tempfile
        import os

        # Check file size first
        file_content = uploaded_file.getvalue()
        file_size_mb = len(file_content) / (1024 * 1024)

        if file_size_mb > 1024:  # 1GB limit
            st.error(f"‚ùå File qu√° l·ªõn: {file_size_mb:.2f}MB. Gi·ªõi h·∫°n: 1024MB")
            return None, None, None

        with tempfile.NamedTemporaryFile(delete=False, suffix='.tex') as tmp_file:
            tmp_file.write(file_content)
            tmp_file_path = tmp_file.name

        # Import parser modules
        import sys
        import os

        # Add src directory to path
        current_dir = os.path.dirname(os.path.abspath(__file__))
        src_dir = os.path.join(current_dir, 'src')
        if src_dir not in sys.path:
            sys.path.insert(0, src_dir)

        from processor.file_reader import FileReader
        from processor.batch_processor import BatchProcessor
        from processor.error_handler import ErrorHandler
        from export.data_validator import DataValidator

        # Process file
        with st.spinner('üîÑ ƒêang x·ª≠ l√Ω file...'):
            # Initialize components
            error_handler = ErrorHandler()

            # Read file and create batches
            file_reader = FileReader(tmp_file_path)

            # Count questions first
            total_questions = file_reader.count_questions()
            if total_questions == 0:
                st.error("‚ùå Kh√¥ng t√¨m th·∫•y c√¢u h·ªèi n√†o trong file")
                os.unlink(tmp_file_path)
                return None, None, None

            st.info(f"üìä T√¨m th·∫•y {total_questions:,} c√¢u h·ªèi")

            # Create batches
            batch_size = min(500, total_questions)  # Smaller batches for Streamlit
            batches = file_reader.split_into_batches(batch_size)

            # Process batches
            progress_bar = st.progress(0)
            status_text = st.empty()

            def progress_callback(processed: int, total: int, elapsed_time: float):
                progress = processed / total if total > 0 else 0
                questions_per_sec = processed / elapsed_time if elapsed_time > 0 else 0
                progress_bar.progress(progress)
                status_text.text(f"‚ö° ƒê√£ x·ª≠ l√Ω: {processed:,}/{total:,} ({progress*100:.1f}%) - {questions_per_sec:.1f} q/s")

            # Process with BatchProcessor
            batch_processor = BatchProcessor(max_workers=1)  # Single worker for Streamlit
            questions, question_codes, errors = batch_processor.process_batches(
                batches,
                progress_callback
            )

            # Validate data
            validator = DataValidator()
            validation_results = validator.validate_all_data(questions, question_codes, [])

            # Auto-generate output files for consistency
            st.info("üíæ ƒêang t·∫°o output files...")

            # Create output directory if not exists
            output_dir = "output"
            os.makedirs(output_dir, exist_ok=True)

            # Export to CSV
            from export.csv_exporter import CSVExporter
            csv_exporter = CSVExporter(output_dir)
            csv_files = csv_exporter.export_all(
                validation_results['valid_questions'],
                validation_results['valid_question_codes'],
                []
            )

            # Export to Excel
            try:
                from export.excel_exporter import ExcelExporter
                excel_exporter = ExcelExporter(output_dir)
                excel_file = excel_exporter.export_all(
                    validation_results['valid_questions'],
                    validation_results['valid_question_codes'],
                    []
                )
                st.success(f"‚úÖ ƒê√£ t·∫°o Excel file: {excel_file}")
            except Exception as excel_error:
                st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o Excel file: {str(excel_error)}")

            # Create error report
            if errors:
                error_file = os.path.join(output_dir, "error.md")
                with open(error_file, 'w', encoding='utf-8') as f:
                    f.write("# Error Report\n\n")
                    for i, error in enumerate(errors, 1):
                        f.write(f"## Error {i}\n{error}\n\n")
                st.info(f"üìã ƒê√£ t·∫°o error report: {error_file}")

            # Create summary
            summary_file = os.path.join(output_dir, "export_summary.md")
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("# Export Summary\n\n")
                f.write(f"- **Total Questions**: {len(questions)}\n")
                f.write(f"- **Valid Questions**: {len(validation_results['valid_questions'])}\n")
                f.write(f"- **Question Codes**: {len(validation_results['valid_question_codes'])}\n")
                f.write(f"- **Errors**: {len(errors)}\n")
                f.write(f"- **Validation Errors**: {len(validation_results.get('validation_errors', []))}\n")

            st.success("‚úÖ ƒê√£ t·∫°o t·∫•t c·∫£ output files!")

            # Clean up temp file
            os.unlink(tmp_file_path)

            # Clear progress indicators
            progress_bar.empty()
            status_text.empty()

            return validation_results['valid_questions'], validation_results['valid_question_codes'], []

    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω file: {str(e)}")
        return None, None, None

def process_local_file(file_path: str):
    """Process a local file directly without upload."""
    try:
        if not os.path.exists(file_path):
            st.error(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {file_path}")
            return None, None, None

        # Import parser modules
        import sys
        current_dir = os.path.dirname(os.path.abspath(__file__))
        src_dir = os.path.join(current_dir, 'src')
        if src_dir not in sys.path:
            sys.path.insert(0, src_dir)

        from processor.file_reader import FileReader
        from processor.batch_processor import BatchProcessor
        from export.data_validator import DataValidator

        # Process file
        with st.spinner('üîÑ ƒêang x·ª≠ l√Ω file local...'):
            file_reader = FileReader(file_path)
            total_questions = file_reader.count_questions()
            st.info(f"üìä T√¨m th·∫•y {total_questions} c√¢u h·ªèi")

            batches = file_reader.split_into_batches(500)
            batch_processor = BatchProcessor(max_workers=1)
            questions, question_codes, errors = batch_processor.process_batches(batches, lambda *args: None)

            validator = DataValidator()
            validation_results = validator.validate_all_data(questions, question_codes, [])

            return validation_results['valid_questions'], validation_results['valid_question_codes'], []

    except Exception as e:
        st.error(f"‚ùå L·ªói x·ª≠ l√Ω file local: {str(e)}")
        return None, None, None

def show_welcome_screen():
    """Show welcome screen when no data is available."""

    st.markdown('<h1 class="main-header">üìö LaTeX Question Parser Dashboard</h1>', unsafe_allow_html=True)

    # Welcome content
    col1, col2, col3 = st.columns([1, 2, 1])

    with col2:
        st.markdown("""
        <div style="text-align: center; padding: 2rem;">
            <h2>üéâ Ch√†o m·ª´ng ƒë·∫øn v·ªõi LaTeX Question Parser!</h2>
            <p style="font-size: 1.2rem; color: #666;">
                H·ªá th·ªëng tr√≠ch xu·∫•t v√† qu·∫£n l√Ω c√¢u h·ªèi t·ª´ file LaTeX
            </p>
        </div>
        """, unsafe_allow_html=True)

        st.markdown("---")

        # Features overview
        st.markdown("### üöÄ T√≠nh nƒÉng ch√≠nh:")

        features = [
            "üìù **Tr√≠ch xu·∫•t c√¢u h·ªèi** t·ª´ file LaTeX (.tex, .md)",
            "üè∑Ô∏è **T·ª± ƒë·ªông t·∫°o tags** d·ª±a tr√™n QuestionCode",
            "üìä **Dashboard th·ªëng k√™** chi ti·∫øt",
            "üíæ **Export CSV/Excel** ƒë·ªÉ s·ª≠ d·ª•ng",
            "üîç **T√¨m ki·∫øm v√† l·ªçc** c√¢u h·ªèi",
            "‚ö° **X·ª≠ l√Ω file l·ªõn** l√™n ƒë·∫øn 1GB"
        ]

        for feature in features:
            st.markdown(f"- {feature}")

        st.markdown("---")

        # Getting started
        st.markdown("### üéØ B·∫Øt ƒë·∫ßu:")

        st.info("""
        **C√°ch 1: Upload File**
        1. Ch·ªçn "üîÑ Upload File" ·ªü sidebar
        2. Upload file .tex ho·∫∑c .md
        3. ƒê·ª£i h·ªá th·ªëng x·ª≠ l√Ω

        **C√°ch 2: File Local**
        1. Ch·ªçn "üìÅ Ch·ªçn File Local" ·ªü sidebar
        2. Ch·ªçn file t·ª´ dropdown
        3. X·ª≠ l√Ω tr·ª±c ti·∫øp
        """)

        # Sample data info
        if os.path.exists("tests/12.OTMN-Lop12-so1-16-24-25.tex"):
            st.success("‚úÖ **File m·∫´u c√≥ s·∫µn**: `tests/12.OTMN-Lop12-so1-16-24-25.tex` (5,790 c√¢u h·ªèi)")

        # Statistics
        st.markdown("### üìà Th·ªëng k√™ h·ªá th·ªëng:")

        col_a, col_b, col_c = st.columns(3)

        with col_a:
            st.metric("Supported Formats", "2", help=".tex, .md files")

        with col_b:
            st.metric("Max File Size", "1GB", help="1024MB upload limit")

        with col_c:
            st.metric("Question Types", "4", help="MC, TF, SA, ES")

        st.markdown("---")

        # Quick actions
        st.markdown("### ‚ö° Quick Actions:")

        col_x, col_y = st.columns(2)

        with col_x:
            if st.button("üöÄ X·ª≠ l√Ω File M·∫´u", help="X·ª≠ l√Ω file m·∫´u c√≥ s·∫µn"):
                if os.path.exists("tests/12.OTMN-Lop12-so1-16-24-25.tex"):
                    st.info("üîÑ ƒêang x·ª≠ l√Ω file m·∫´u... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)")
                    st.rerun()
                else:
                    st.error("‚ùå File m·∫´u kh√¥ng t·ªìn t·∫°i")

        with col_y:
            if st.button("üìñ Xem H∆∞·ªõng D·∫´n", help="Xem h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng"):
                st.info("üí° H∆∞·ªõng d·∫´n: S·ª≠ d·ª•ng sidebar ƒë·ªÉ upload file ho·∫∑c ch·ªçn file local")

def main():
    """Main Streamlit app."""

    # Header
    st.markdown('<h1 class="main-header">üìö LaTeX Question Parser Dashboard</h1>', unsafe_allow_html=True)

    # File upload section
    st.sidebar.header("üìÅ Upload File")
    st.sidebar.info("üí° **Gi·ªõi h·∫°n upload**: 1GB (1024MB)")

    # Upload method selection
    upload_method = st.sidebar.radio(
        "Ch·ªçn ph∆∞∆°ng th·ª©c upload:",
        ["üîÑ Upload File", "üìÅ Ch·ªçn File Local"],
        help="Upload File: Upload t·ª´ m√°y t√≠nh. File Local: Ch·ªçn file ƒë√£ c√≥ trong th∆∞ m·ª•c."
    )

    uploaded_file = None
    local_file_path = None

    if upload_method == "üîÑ Upload File":
        uploaded_file = st.sidebar.file_uploader(
            "Ch·ªçn file LaTeX ƒë·ªÉ tr√≠ch xu·∫•t",
            type=['tex', 'md'],
            help="Upload file .tex ho·∫∑c .md ƒë·ªÉ tr√≠ch xu·∫•t c√¢u h·ªèi. H·ªó tr·ª£ file l√™n ƒë·∫øn 1GB.",
            key="file_uploader_main"
        )
    else:
        # Local file selection
        st.sidebar.markdown("**üìÅ File Local Available:**")

        # Look for .tex files in current directory and subdirectories
        available_files = []
        for root, dirs, files in os.walk('.'):
            for file in files:
                if file.endswith(('.tex', '.md')):
                    file_path = os.path.join(root, file)
                    file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                    available_files.append((file_path, file_size))

        if available_files:
            file_options = [f"{path} ({size:.1f}MB)" for path, size in available_files]
            selected_file = st.sidebar.selectbox(
                "Ch·ªçn file:",
                ["-- Ch·ªçn file --"] + file_options
            )

            if selected_file != "-- Ch·ªçn file --":
                # Extract file path from selection
                local_file_path = selected_file.split(" (")[0]
                st.sidebar.success(f"‚úÖ ƒê√£ ch·ªçn: {os.path.basename(local_file_path)}")
        else:
            st.sidebar.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file .tex ho·∫∑c .md n√†o")

    # Process uploaded file or load existing data
    if uploaded_file is not None or local_file_path is not None:
        if uploaded_file is not None:
            # Handle uploaded file
            file_size_mb = len(uploaded_file.getvalue()) / (1024 * 1024)
            st.sidebar.success(f"‚úÖ File uploaded: {uploaded_file.name}")
            st.sidebar.info(f"üìä File size: {file_size_mb:.2f} MB")

            # Show processing message for large files
            if file_size_mb > 50:
                st.sidebar.warning("‚è≥ File l·ªõn - qu√° tr√¨nh x·ª≠ l√Ω c√≥ th·ªÉ m·∫•t v√†i ph√∫t...")

            questions_list, codes_list, tags_list = process_uploaded_file(uploaded_file)
        else:
            # Handle local file
            file_size_mb = os.path.getsize(local_file_path) / (1024 * 1024)
            st.sidebar.info(f"üìä File size: {file_size_mb:.2f} MB")

            # Show processing message for large files
            if file_size_mb > 50:
                st.sidebar.warning("‚è≥ File l·ªõn - qu√° tr√¨nh x·ª≠ l√Ω c√≥ th·ªÉ m·∫•t v√†i ph√∫t...")

            questions_list, codes_list, tags_list = process_local_file(local_file_path)

        if questions_list is not None:
            # Convert to DataFrame
            questions_df = pd.DataFrame([q.to_csv_dict() for q in questions_list])
            codes_df = pd.DataFrame([c.to_csv_dict() for c in codes_list]) if codes_list else pd.DataFrame()
            tags_df = pd.DataFrame()

            st.sidebar.success(f"‚úÖ ƒê√£ tr√≠ch xu·∫•t {len(questions_list)} c√¢u h·ªèi")
        else:
            st.error("‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω file. Vui l√≤ng th·ª≠ l·∫°i.")
            return
    else:
        # Load existing data
        questions_df, codes_df, tags_df = load_data()

        if questions_df is None or len(questions_df) == 0:
            # Show welcome screen when no data
            show_welcome_screen()
            return
    
    # Create filters
    filters = create_filters(questions_df, codes_df)
    
    # Apply filters
    filtered_df = apply_filters(questions_df, codes_df, filters)
    
    # Main content tabs
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["üìä Dashboard", "üìù Questions", "üìã Question Codes", "üè∑Ô∏è Tags", "‚ùå Error Questions", "‚è≥ Pending Questions"])

    with tab1:
        display_statistics(questions_df, filtered_df)

    with tab2:
        st.subheader(f"üìù Questions ({len(filtered_df)} found)")

        # Show available columns info
        if len(filtered_df) > 0:
            with st.expander("‚ÑπÔ∏è Available Data Fields"):
                st.write("**Fields extracted for each question:**")
                cols_info = st.columns(3)

                all_columns = list(filtered_df.columns)
                for i, col in enumerate(all_columns):
                    with cols_info[i % 3]:
                        # Count non-null values
                        non_null_count = filtered_df[col].notna().sum()
                        total_count = len(filtered_df)
                        percentage = (non_null_count / total_count * 100) if total_count > 0 else 0

                        st.write(f"**{col}:** {non_null_count}/{total_count} ({percentage:.1f}%)")

        if len(filtered_df) > 0:
            # Pagination
            items_per_page = st.selectbox("Items per page", [10, 25, 50, 100], index=1)
            total_pages = (len(filtered_df) - 1) // items_per_page + 1
            
            if total_pages > 1:
                page = st.selectbox("Page", range(1, total_pages + 1))
                start_idx = (page - 1) * items_per_page
                end_idx = start_idx + items_per_page
                page_df = filtered_df.iloc[start_idx:end_idx]
            else:
                page_df = filtered_df
            
            # Display questions
            for idx, row in page_df.iterrows():
                # Create expander title with key info
                status_emoji = "‚úÖ" if row.get('status') == 'ACTIVE' else "‚è≥" if row.get('status') == 'PENDING' else "‚ùì"
                type_emoji = {"MC": "üîò", "TF": "‚úì‚ùå", "SA": "üìù", "ES": "üìÑ"}.get(row.get('type', ''), "‚ùì")

                expander_title = f"{status_emoji} {type_emoji} Question {row.get('id', idx)} - {row.get('type', 'Unknown')} - {row.get('status', 'Unknown')}"

                with st.expander(expander_title):
                    # Raw Content section - preserve LaTeX commands
                    st.markdown("### üîç Raw Content")
                    raw_content = clean_raw_content_display(row.get('rawContent', 'No raw content available'))
                    st.text_area("Raw Content", raw_content, height=120, disabled=True, key=f"raw_{idx}")

                    # Processed Content section
                    st.markdown("### üìù Content")
                    content = clean_text_display(row.get('content', 'No content available'))
                    st.text_area("Content", content, height=100, disabled=True, key=f"content_{idx}")

                    # Source section
                    st.markdown("### üìö Source")
                    source = clean_text_display(row.get('source', 'No source available'))
                    st.text_area("Source", source, height=60, disabled=True, key=f"source_{idx}")

                    # Answers section
                    st.markdown("### üìã Answers")
                    answers_text = format_answers_display(row.get('answers', 'No answers available'))
                    st.text_area("Answers", answers_text, height=120, disabled=True, key=f"answers_{idx}")

                    # Correct Answer section
                    st.markdown("### ‚úÖ Correct Answer")
                    correct_answer = clean_text_display(row.get('correctAnswer', 'No correct answer available'))
                    st.text_area("Correct Answer", correct_answer, height=60, disabled=True, key=f"correct_{idx}")

                    # Solution section - preserve LaTeX commands
                    st.markdown("### üí° Solution")
                    solution = clean_raw_content_display(row.get('solution', 'No solution available'))
                    st.text_area("Solution", solution, height=150, disabled=True, key=f"solution_{idx}")

                    # Generated Tags section
                    st.markdown("### üè∑Ô∏è Generated Tags")
                    generated_tags = row.get('generatedTags', 'No tags generated')
                    if generated_tags and generated_tags.strip():
                        # Display tags as badges
                        tags_list = [tag.strip() for tag in generated_tags.split(';') if tag.strip()]
                        if tags_list:
                            # Display tags in a nice format
                            tag_text = " | ".join([f"üè∑Ô∏è {tag}" for tag in tags_list])
                            st.markdown(f"**{tag_text}**")
                        else:
                            st.info("No tags available")
                    else:
                        st.info("No tags generated")

                    # Metadata in columns
                    st.markdown("### üìä Metadata")
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.markdown("**Basic Info:**")
                        st.write(f"üÜî **ID:** {clean_text_display(row.get('id', 'N/A'))}")
                        st.write(f"üìù **Type:** {clean_text_display(row.get('type', 'N/A'))}")

                        status = row.get('status', 'N/A')
                        if status == 'ACTIVE':
                            st.markdown('üü¢ **Status:** <span class="status-active">ACTIVE</span>', unsafe_allow_html=True)
                        elif status == 'PENDING':
                            st.markdown('üü° **Status:** <span class="status-pending">PENDING</span>', unsafe_allow_html=True)
                        else:
                            st.write(f"‚ùì **Status:** {status}")

                        st.write(f"‚≠ê **Difficulty:** {clean_text_display(row.get('difficulty', 'N/A'))}")
                        st.write(f"üìÅ **Source:** {clean_text_display(row.get('source', 'N/A'))}")

                    with col2:
                        st.markdown("**Usage & Quality:**")
                        st.write(f"üë§ **Creator:** {clean_text_display(row.get('creator', 'N/A'))}")
                        st.write(f"üìä **Usage Count:** {row.get('usageCount', 0)}")
                        st.write(f"üí¨ **Feedback:** {row.get('feedback', 0)}")
                        st.write(f"üî¢ **Subcount:** {clean_text_display(row.get('subcount', 'N/A'))}")
                        st.write(f"üè∑Ô∏è **Tags:** {clean_text_display(row.get('tags', 'N/A'))}")

                    with col3:
                        st.markdown("**System & Timestamps:**")
                        st.write(f"üîó **Question Code ID:** {clean_text_display(row.get('questionCodeId', 'N/A'))}")

                        # Timestamps
                        created_at = clean_text_display(row.get('created_at', 'N/A'))
                        updated_at = clean_text_display(row.get('updated_at', 'N/A'))
                        st.write(f"üìÖ **Created:** {created_at}")
                        st.write(f"üîÑ **Updated:** {updated_at}")

                        # Additional fields that might exist
                        for field in ['answers', 'correctAnswer']:
                            if field in row and row[field] and str(row[field]).strip():
                                field_name = field.replace('Answer', ' Answer').replace('correct', 'Correct')
                                preview = clean_text_display(str(row[field])[:50] + "..." if len(str(row[field])) > 50 else str(row[field]))
                                st.write(f"üìã **{field_name}:** {preview}")

                    # All fields section (collapsible)
                    with st.expander("üìã All Fields (Complete Data)"):
                        st.markdown("**All extracted fields for this question:**")

                        # Display all columns in a structured way
                        all_cols = st.columns(2)
                        col_idx = 0

                        for column in row.index:
                            if column not in ['rawContent']:  # Skip raw content as it's shown separately
                                value = clean_text_display(row[column])

                                # Truncate very long values for overview
                                if len(str(value)) > 100:
                                    display_value = str(value)[:100] + "..."
                                else:
                                    display_value = str(value)

                                with all_cols[col_idx % 2]:
                                    st.write(f"**{column}:** {display_value}")

                                col_idx += 1

                    # Raw content section (collapsible)
                    if row.get('rawContent'):
                        with st.expander("üîç View Raw LaTeX Content"):
                            raw_content = str(row.get('rawContent', ''))
                            st.code(raw_content, language='latex')
        else:
            st.info("No questions found with current filters.")
    
    with tab3:
        st.subheader("üìã Question Codes")
        if not codes_df.empty:
            st.dataframe(codes_df, use_container_width=True)
        else:
            st.info("No question codes available.")
    
    with tab4:
        st.subheader("üè∑Ô∏è Tags")
        if not tags_df.empty:
            st.dataframe(tags_df, use_container_width=True)
        else:
            st.info("No tags available.")

    with tab5:
        st.subheader("‚ùå Error Questions")

        # Load error data if exists
        error_file = "output/error.md"
        if os.path.exists(error_file):
            with open(error_file, 'r', encoding='utf-8') as f:
                error_content = f.read()

            if error_content.strip():
                # Parse error content to extract statistics
                lines = error_content.split('\n')
                total_errors = 0
                validation_errors = 0
                malformed_questions = 0

                for line in lines:
                    if line.startswith('- Total Errors:'):
                        total_errors = int(line.split(':')[1].strip())
                    elif line.startswith('- Validation Errors:'):
                        validation_errors = int(line.split(':')[1].strip())
                    elif line.startswith('- Malformed Questions:'):
                        malformed_questions = int(line.split(':')[1].strip())

                # Display metrics
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total Errors", total_errors)
                with col2:
                    st.metric("Validation Errors", validation_errors)
                with col3:
                    st.metric("Malformed Questions", malformed_questions)

                st.markdown("---")

                # Display full error report with proper formatting
                st.markdown("### üìã Complete Error Report")
                st.markdown("**Note**: Each error question includes complete `\\begin{ex}...\\end{ex}` blocks for easy identification.")

                # Use code block for better LaTeX display
                st.code(error_content, language="markdown")

                # Download button for error report
                st.download_button(
                    label="üì• Download Error Report",
                    data=error_content,
                    file_name="error_report.md",
                    mime="text/markdown"
                )
            else:
                st.success("üéâ No errors found!")
        else:
            st.info("No error file available. Run the parser to generate error reports.")

    with tab6:
        st.subheader("‚è≥ Pending Questions")

        # Filter questions with PENDING status
        if not questions_df.empty:
            pending_questions = questions_df[questions_df['status'] == 'PENDING']

            if not pending_questions.empty:
                st.warning(f"Found {len(pending_questions)} questions with PENDING status")

                # Display pending questions
                for idx, (_, row) in enumerate(pending_questions.iterrows()):
                    status_emoji = "‚è≥"
                    type_emoji = {"MC": "üîò", "TF": "‚úì‚ùå", "SA": "üìù", "ES": "üìÑ"}.get(row.get('type', ''), "‚ùì")

                    expander_title = f"{status_emoji} {type_emoji} Question {row.get('id', idx)} - {row.get('type', 'Unknown')} - PENDING"

                    with st.expander(expander_title):
                        # Raw Content section
                        st.markdown("### üîç Raw Content")
                        raw_content = clean_raw_content_display(row.get('rawContent', 'No raw content available'))
                        st.text_area("Raw Content", raw_content, height=120, disabled=True, key=f"pending_raw_{idx}")

                        # Processed Content section
                        st.markdown("### üìù Content")
                        content = clean_text_display(row.get('content', 'No content available'))
                        st.text_area("Content", content, height=100, disabled=True, key=f"pending_content_{idx}")

                        # Show why it's pending (if available)
                        st.markdown("### ‚ö†Ô∏è Pending Reason")
                        st.info("This question requires manual review before being marked as ACTIVE.")
            else:
                st.success("üéâ No pending questions found!")
        else:
            st.info("No questions data available.")

    # Footer with export options
    st.markdown("---")
    st.subheader("üì• Export Options")

    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("üìä Download Filtered CSV"):
            csv = filtered_df.to_csv(index=False)
            st.download_button(
                label="Download CSV",
                data=csv,
                file_name=f"filtered_questions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )

    with col2:
        if os.path.exists("output/questions_export.xlsx"):
            with open("output/questions_export.xlsx", "rb") as file:
                st.download_button(
                    label="üìä Download Full Excel",
                    data=file.read(),
                    file_name="questions_export.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

    with col3:
        st.info("üí° Use filters to narrow down results before exporting")

if __name__ == "__main__":
    main()
